{"version":3,"sources":["../../../src/generated/tree/ExtraTreeRegressor.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An extremely randomized tree regressor.\n\n  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max\\_features` randomly selected features and the best split among those is chosen. When `max\\_features` is set 1, this amounts to building a totally random decision tree.\n\n  Warning: Extra-trees should only be used within ensemble methods.\n\n  Read more in the [User Guide](../tree.html#tree).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html)\n */\nexport class ExtraTreeRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The function to measure the quality of a split. Supported criteria are “squared\\_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman\\_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute\\_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.\n\n      @defaultValue `'squared_error'`\n     */\n    criterion?: 'squared_error' | 'friedman_mse' | 'absolute_error' | 'poisson'\n\n    /**\n      The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n\n      @defaultValue `'random'`\n     */\n    splitter?: 'random' | 'best'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n\n      @defaultValue `1`\n     */\n    max_features?: number | 'sqrt'\n\n    /**\n      Used to pick randomly the `max\\_features` used at each split. See [Glossary](../../glossary.html#term-random_state) for details.\n     */\n    random_state?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Grow a tree with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n  }) {\n    this.id = `ExtraTreeRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'ExtraTreeRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.tree import ExtraTreeRegressor\ntry: bridgeExtraTreeRegressor\nexcept NameError: bridgeExtraTreeRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_ExtraTreeRegressor = {'criterion': ${\n      this.opts['criterion'] ?? undefined\n    }, 'splitter': ${this.opts['splitter'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}}\n\nctor_ExtraTreeRegressor = {k: v for k, v in ctor_ExtraTreeRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeExtraTreeRegressor[${this.id}] = ExtraTreeRegressor(**ctor_ExtraTreeRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeExtraTreeRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Return the index of the leaf that each sample is predicted as.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<ArrayLike> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreeRegressor must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_apply = {k: v for k, v in pms_ExtraTreeRegressor_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_apply = bridgeExtraTreeRegressor[${this.id}].apply(**pms_ExtraTreeRegressor_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_apply.tolist() if hasattr(res_ExtraTreeRegressor_apply, 'tolist') else res_ExtraTreeRegressor_apply`\n  }\n\n  /**\n    Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n    See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details on the pruning process.\n   */\n  async cost_complexity_pruning_path(opts: {\n    /**\n      The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels) as integers or strings.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before cost_complexity_pruning_path()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreeRegressor_cost_complexity_pruning_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreeRegressor_cost_complexity_pruning_path = {k: v for k, v in pms_ExtraTreeRegressor_cost_complexity_pruning_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_cost_complexity_pruning_path = bridgeExtraTreeRegressor[${this.id}].cost_complexity_pruning_path(**pms_ExtraTreeRegressor_cost_complexity_pruning_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_cost_complexity_pruning_path.tolist() if hasattr(res_ExtraTreeRegressor_cost_complexity_pruning_path, 'tolist') else res_ExtraTreeRegressor_cost_complexity_pruning_path`\n  }\n\n  /**\n    Return the decision path in the tree.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_decision_path = {k: v for k, v in pms_ExtraTreeRegressor_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_decision_path = bridgeExtraTreeRegressor[${this.id}].decision_path(**pms_ExtraTreeRegressor_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_decision_path.tolist() if hasattr(res_ExtraTreeRegressor_decision_path, 'tolist') else res_ExtraTreeRegressor_decision_path`\n  }\n\n  /**\n    Build a decision tree regressor from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (real numbers). Use `dtype=np.float64` and `order='C'` for maximum efficiency.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.\n     */\n    sample_weight?: ArrayLike\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreeRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_fit = {k: v for k, v in pms_ExtraTreeRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_fit = bridgeExtraTreeRegressor[${this.id}].fit(**pms_ExtraTreeRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_fit.tolist() if hasattr(res_ExtraTreeRegressor_fit, 'tolist') else res_ExtraTreeRegressor_fit`\n  }\n\n  /**\n    Return the depth of the decision tree.\n\n    The depth of a tree is the maximum distance between the root and any leaf.\n   */\n  async get_depth(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreeRegressor must call init() before get_depth()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_get_depth = {}\n\npms_ExtraTreeRegressor_get_depth = {k: v for k, v in pms_ExtraTreeRegressor_get_depth.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_get_depth = bridgeExtraTreeRegressor[${this.id}].get_depth(**pms_ExtraTreeRegressor_get_depth)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_get_depth.tolist() if hasattr(res_ExtraTreeRegressor_get_depth, 'tolist') else res_ExtraTreeRegressor_get_depth`\n  }\n\n  /**\n    Get metadata routing of this object.\n\n    Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n   */\n  async get_metadata_routing(opts: {\n    /**\n      A [`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest \"sklearn.utils.metadata_routing.MetadataRequest\") encapsulating routing information.\n     */\n    routing?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before get_metadata_routing()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreeRegressor_get_metadata_routing = {'routing': ${\n      opts['routing'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_get_metadata_routing = {k: v for k, v in pms_ExtraTreeRegressor_get_metadata_routing.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_get_metadata_routing = bridgeExtraTreeRegressor[${this.id}].get_metadata_routing(**pms_ExtraTreeRegressor_get_metadata_routing)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_get_metadata_routing.tolist() if hasattr(res_ExtraTreeRegressor_get_metadata_routing, 'tolist') else res_ExtraTreeRegressor_get_metadata_routing`\n  }\n\n  /**\n    Return the number of leaves of the decision tree.\n   */\n  async get_n_leaves(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before get_n_leaves()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_get_n_leaves = {}\n\npms_ExtraTreeRegressor_get_n_leaves = {k: v for k, v in pms_ExtraTreeRegressor_get_n_leaves.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_get_n_leaves = bridgeExtraTreeRegressor[${this.id}].get_n_leaves(**pms_ExtraTreeRegressor_get_n_leaves)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_get_n_leaves.tolist() if hasattr(res_ExtraTreeRegressor_get_n_leaves, 'tolist') else res_ExtraTreeRegressor_get_n_leaves`\n  }\n\n  /**\n    Predict class or regression value for X.\n\n    For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<ArrayLike> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreeRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_predict = {k: v for k, v in pms_ExtraTreeRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_predict = bridgeExtraTreeRegressor[${this.id}].predict(**pms_ExtraTreeRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_predict.tolist() if hasattr(res_ExtraTreeRegressor_predict, 'tolist') else res_ExtraTreeRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreeRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreeRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreeRegressor_score = {k: v for k, v in pms_ExtraTreeRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_score = bridgeExtraTreeRegressor[${this.id}].score(**pms_ExtraTreeRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_score.tolist() if hasattr(res_ExtraTreeRegressor_score, 'tolist') else res_ExtraTreeRegressor_score`\n  }\n\n  /**\n    Request metadata passed to the `fit` method.\n\n    Note that this method is only relevant if `enable\\_metadata\\_routing=True` (see [`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config \"sklearn.set_config\")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n\n    The options for each parameter are:\n   */\n  async set_fit_request(opts: {\n    /**\n      Metadata routing for `check\\_input` parameter in `fit`.\n     */\n    check_input?: string | boolean\n\n    /**\n      Metadata routing for `sample\\_weight` parameter in `fit`.\n     */\n    sample_weight?: string | boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before set_fit_request()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreeRegressor_set_fit_request = {'check_input': ${\n      opts['check_input'] ?? undefined\n    }, 'sample_weight': ${opts['sample_weight'] ?? undefined}}\n\npms_ExtraTreeRegressor_set_fit_request = {k: v for k, v in pms_ExtraTreeRegressor_set_fit_request.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_set_fit_request = bridgeExtraTreeRegressor[${this.id}].set_fit_request(**pms_ExtraTreeRegressor_set_fit_request)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_set_fit_request.tolist() if hasattr(res_ExtraTreeRegressor_set_fit_request, 'tolist') else res_ExtraTreeRegressor_set_fit_request`\n  }\n\n  /**\n    Request metadata passed to the `predict` method.\n\n    Note that this method is only relevant if `enable\\_metadata\\_routing=True` (see [`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config \"sklearn.set_config\")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n\n    The options for each parameter are:\n   */\n  async set_predict_request(opts: {\n    /**\n      Metadata routing for `check\\_input` parameter in `predict`.\n     */\n    check_input?: string | boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before set_predict_request()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreeRegressor_set_predict_request = {'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_set_predict_request = {k: v for k, v in pms_ExtraTreeRegressor_set_predict_request.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_set_predict_request = bridgeExtraTreeRegressor[${this.id}].set_predict_request(**pms_ExtraTreeRegressor_set_predict_request)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_set_predict_request.tolist() if hasattr(res_ExtraTreeRegressor_set_predict_request, 'tolist') else res_ExtraTreeRegressor_set_predict_request`\n  }\n\n  /**\n    Request metadata passed to the `score` method.\n\n    Note that this method is only relevant if `enable\\_metadata\\_routing=True` (see [`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config \"sklearn.set_config\")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n\n    The options for each parameter are:\n   */\n  async set_score_request(opts: {\n    /**\n      Metadata routing for `sample\\_weight` parameter in `score`.\n     */\n    sample_weight?: string | boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before set_score_request()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreeRegressor_set_score_request = {'sample_weight': ${\n      opts['sample_weight'] ?? undefined\n    }}\n\npms_ExtraTreeRegressor_set_score_request = {k: v for k, v in pms_ExtraTreeRegressor_set_score_request.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreeRegressor_set_score_request = bridgeExtraTreeRegressor[${this.id}].set_score_request(**pms_ExtraTreeRegressor_set_score_request)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreeRegressor_set_score_request.tolist() if hasattr(res_ExtraTreeRegressor_set_score_request, 'tolist') else res_ExtraTreeRegressor_set_score_request`\n  }\n\n  /**\n    The inferred value of max\\_features.\n   */\n  get max_features_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before accessing max_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreeRegressor_max_features_ = bridgeExtraTreeRegressor[${this.id}].max_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreeRegressor_max_features_.tolist() if hasattr(attr_ExtraTreeRegressor_max_features_, 'tolist') else attr_ExtraTreeRegressor_max_features_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreeRegressor_n_features_in_ = bridgeExtraTreeRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreeRegressor_n_features_in_.tolist() if hasattr(attr_ExtraTreeRegressor_n_features_in_, 'tolist') else attr_ExtraTreeRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreeRegressor_feature_names_in_ = bridgeExtraTreeRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreeRegressor_feature_names_in_.tolist() if hasattr(attr_ExtraTreeRegressor_feature_names_in_, 'tolist') else attr_ExtraTreeRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreeRegressor_n_outputs_ = bridgeExtraTreeRegressor[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreeRegressor_n_outputs_.tolist() if hasattr(attr_ExtraTreeRegressor_n_outputs_, 'tolist') else attr_ExtraTreeRegressor_n_outputs_`\n    })()\n  }\n\n  /**\n    The underlying Tree object. Please refer to `help(sklearn.tree.\\_tree.Tree)` for attributes of Tree object and [Understanding the decision tree structure](../../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) for basic usage of these attributes.\n   */\n  get tree_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreeRegressor must call init() before accessing tree_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreeRegressor_tree_ = bridgeExtraTreeRegressor[${this.id}].tree_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreeRegressor_tree_.tolist() if hasattr(attr_ExtraTreeRegressor_tree_, 'tolist') else attr_ExtraTreeRegressor_tree_`\n    })()\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAeZ,IAAM,qBAAN,MAAyB;AAAA,EAQ9B,YAAY,MAyET;AA5EH,0BAA0B;AAC1B,uBAAuB;AA4ErB,SAAK,KAAK,qBAAqB,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC/D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,6CACb,KAAK,KAAK,WAAW,KAAK,uBACX,KAAK,KAAK,UAAU,KAAK,wBACxC,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,2BAE7B,KAAK,KAAK,cAAc,KAAK,oCAE7B,KAAK,KAAK,uBAAuB,KAAK,6BAEtC,KAAK,KAAK,gBAAgB,KAAK,wBACf,KAAK,KAAK,WAAW,KAAK;AAAA;AAAA;AAI5C,UAAM,KAAK,IACR,8BAA8B,KAAK;AAEtC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,kCAAkC,KAAK;AAEtD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAYW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,6BAA6B,MAelB;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,0EACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,oFAAoF,KAAK;AAG5F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAYQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAsBO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM,oCAChC,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU,MAAwB;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,qBAAqB,MAKV;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,SAAS,KAAK;AAAA;AAAA;AAMrB,UAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,MAAwB;AACzC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAYS;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,gBAAgB,MAUL;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,aAAa,KAAK,4BACH,KAAK,eAAe,KAAK;AAAA;AAAA;AAK/C,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,oBAAoB,MAKT;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,kEACD,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,kBAAkB,MAKP;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,kEACD,KAAK,eAAe,KAAK;AAAA;AAAA;AAM3B,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;","names":[]}